{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning in linear regression is the learning of weights/params for each predictor variable. Training a linear regression model involves finding the best weights to optimize loss(difference of the predicted model from the ground truth model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 instances of the input variables( temp, rainfall, humidity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two target variables: Apples and oranges yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert the inputs and target arrays to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Random Initialization of weights\n",
    "\n",
    "\n",
    "We initialize w and b with elements randomly picked from a gaussian distribution with mean 0 and std 1 using: \n",
    "#### torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(2,3, requires_grad= True) # number of weights = number of predictor variables times  target variables. In this case, 3 x 2 \n",
    "b =  torch.randn(2, requires_grad= True) # number of biases =  number of target variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0807, -0.9576, -1.2509],\n",
      "        [ 0.0829, -0.1652, -0.5678]], requires_grad=True)\n",
      "tensor([-0.3241,  1.1236], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x@w.t() + b  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x@w :\n",
    "@ is used for matrix multiplication between the two matrices.\n",
    "\n",
    "#### .t() transposes w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Predicting the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluating performance of model using loss function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First, let's take a glance between our targets and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-197.1626,  -28.3126],\n",
      "        [-262.9934,  -42.2148],\n",
      "        [-295.2137,  -46.7383],\n",
      "        [-198.0163,  -18.5376],\n",
      "        [-254.3834,  -48.7666]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions obviously way off from the ground truth. We have to minimize this difference between the two a.k.a. loss/error.\n",
    "\n",
    "In linear regression,the loss is characterized in terms of mean squared error(MSE).\n",
    "\n",
    "MSE is the average squared difference of the corresponding elements from the prediciton tensor to the target tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function that calculates MSE. \n",
    "def mse(t1, t2):\n",
    "    diff = t1-t2 \n",
    "    return torch.sum(diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.sum(...) sums up all elements in a tensor \n",
    "#### .numel() calculates the number of elements in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate the loss of our model. \n",
    "loss = mse(prediction, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty high loss or deviation from the ground truth. We need a way to minimize this loss, which is where gradients come into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Computing gradients of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .backward() \n",
    "computes the gradient of loss w.r.t all the elements \n",
    "that were used to compute it(the ones that had requires_grad set to True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .grad\n",
    "\n",
    "displays the derivative of a function w.r.t. the particular element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #computing the derivative of the loss function w.r.t b and w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss/dw: tensor([[-26584.3984, -29301.4922, -18016.6836],\n",
      "        [-10617.4648, -12353.6982,  -7523.3125]])\n",
      "dloss/db: tensor([-317.7539, -128.9140])\n"
     ]
    }
   ],
   "source": [
    "#display gradients \n",
    "print('dloss/dw:', w.grad) \n",
    "print('dloss/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our objective is to find the set of weights where the loss is the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grad element is positive (slope is positive), increasing the element's value will increase the loss. \n",
    "Decreasing the element's value will decrease the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grad element is negative (slope is negative), increasing the element's value will decrease the loss. \n",
    "Decreasing the element's value will increase the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
