{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning in linear regression is the learning of weights/params for each predictor variable. Training a linear regression model involves finding the best weights to optimize loss(difference of the predicted model from the ground truth model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 instances of the input variables( temp, rainfall, humidity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two target variables: Apples and oranges yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert the inputs and target arrays to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Random Initialization of weights\n",
    "\n",
    "\n",
    "We initialize w and b with elements randomly picked from a gaussian distribution with mean 0 and std 1 using: \n",
    "#### torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(2,3, requires_grad= True) # number of weights = number of predictor variables times  target variables. In this case, 3 x 2 \n",
    "b =  torch.randn(2, requires_grad= True) # number of biases =  number of target variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6969, -0.2638, -0.1734],\n",
      "        [ 1.6521,  0.6281, -0.0261]], requires_grad=True)\n",
      "tensor([1.3213, 0.6752], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x@w.t() + b  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x@w :\n",
    "@ is used for matrix multiplication between the two matrices.\n",
    "\n",
    "#### .t() transposes w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Predicting the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluating performance of model using loss function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First, let's take a glance between our targets and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.0649, 162.2357],\n",
      "        [ 30.4277, 204.6144],\n",
      "        [ 16.5464, 227.0556],\n",
      "        [ 54.6472, 195.2283],\n",
      "        [ 11.9444, 173.1367]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions obviously way off from the ground truth. We have to minimize this difference between the two a.k.a. loss/error.\n",
    "\n",
    "In linear regression,the loss is characterized in terms of mean squared error(MSE).\n",
    "\n",
    "MSE is the average squared difference of the corresponding elements from the prediciton tensor to the target tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function that calculates MSE. \n",
    "def mse(t1, t2):\n",
    "    diff = t1-t2 \n",
    "    return torch.sum(diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.sum(...) sums up all elements in a tensor \n",
    "#### .numel() calculates the number of elements in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate the loss of our model. \n",
    "loss = mse(prediction, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty high loss or deviation from the ground truth. We need a way to minimize this loss, which is where gradients come into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Computing gradients of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .backward() \n",
    "computes the gradient of loss w.r.t all the elements \n",
    "that were used to compute it(the ones that had requires_grad set to True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .grad\n",
    "\n",
    "displays the derivative of a function w.r.t. the particular element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #computing the derivative of the loss function w.r.t b and w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss/dw: tensor([[-3716.1250, -5491.0605, -3117.8179],\n",
      "        [ 8843.9365,  7980.4521,  5139.3408]])\n",
      "dloss/db: tensor([-48.0739, 100.4542])\n"
     ]
    }
   ],
   "source": [
    "#display gradients \n",
    "print('dloss/dw:', w.grad) \n",
    "print('dloss/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our objective is to find the set of weights where the loss is the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grad element is positive (slope is positive), increasing the element's value will increase the loss. \n",
    "Decreasing the element's value will decrease the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grad element is negative (slope is negative), increasing the element's value will decrease the loss. \n",
    "Decreasing the element's value will increase the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adjusting the value of the weights in proportion to the value of the gradients,\n",
    "we can update loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime we run .backward, the new gradient value gets added to the existing gradient values.\n",
    "We need to reset grad values to zero during each training epoch before calculating new gradients for the updated loss function. \n",
    "We can do that by using the .zero_() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Updating weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to reimplement step 3. through step 5. to update w and b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3716.1250, -5491.0605, -3117.8179],\n",
      "        [ 8843.9365,  7980.4521,  5139.3408]])\n",
      "tensor([-48.0739, 100.4542])\n"
     ]
    }
   ],
   "source": [
    "prediction = model(inputs) #step 3 \n",
    "loss = mse(prediction, targets)#step 4\n",
    "loss.backward() #step 5\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7930.5322, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss) #prints loss before adjusting weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.no_grad() \n",
    "deactivates autograd engine. Eventually it will reduce the memory usage and speed up computations. We notify pytorch that we aren't going to automatically track or modify the grads based on the updated weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting weights and reset gradients. \n",
    "#learning rate has been set to 10^-5\n",
    "with torch.no_grad(): \n",
    "    w-= w.grad*1e-5 \n",
    "    b-= b.grad*1e-5\n",
    "#resetting grads to 0\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5907.3530, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#updating loss\n",
    "prediction = model(inputs) #step 3 \n",
    "loss = mse(prediction, targets)#step 4\n",
    "print(loss) #new loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss has gone down significantly, meaning our model is improving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to train the model for multiple epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5907.3530, grad_fn=<DivBackward0>)\n",
      "tensor(4537.1553, grad_fn=<DivBackward0>)\n",
      "tensor(3607.0786, grad_fn=<DivBackward0>)\n",
      "tensor(2973.6763, grad_fn=<DivBackward0>)\n",
      "tensor(2540.2820, grad_fn=<DivBackward0>)\n",
      "tensor(2241.7527, grad_fn=<DivBackward0>)\n",
      "tensor(2034.1881, grad_fn=<DivBackward0>)\n",
      "tensor(1888.0032, grad_fn=<DivBackward0>)\n",
      "tensor(1783.2605, grad_fn=<DivBackward0>)\n",
      "tensor(1706.5232, grad_fn=<DivBackward0>)\n",
      "tensor(1648.7357, grad_fn=<DivBackward0>)\n",
      "tensor(1603.7944, grad_fn=<DivBackward0>)\n",
      "tensor(1567.5848, grad_fn=<DivBackward0>)\n",
      "tensor(1537.3339, grad_fn=<DivBackward0>)\n",
      "tensor(1511.1716, grad_fn=<DivBackward0>)\n",
      "tensor(1487.8372, grad_fn=<DivBackward0>)\n",
      "tensor(1466.4800, grad_fn=<DivBackward0>)\n",
      "tensor(1446.5251, grad_fn=<DivBackward0>)\n",
      "tensor(1427.5856, grad_fn=<DivBackward0>)\n",
      "tensor(1409.3990, grad_fn=<DivBackward0>)\n",
      "tensor(1391.7874, grad_fn=<DivBackward0>)\n",
      "tensor(1374.6309, grad_fn=<DivBackward0>)\n",
      "tensor(1357.8469, grad_fn=<DivBackward0>)\n",
      "tensor(1341.3795, grad_fn=<DivBackward0>)\n",
      "tensor(1325.1901, grad_fn=<DivBackward0>)\n",
      "tensor(1309.2516, grad_fn=<DivBackward0>)\n",
      "tensor(1293.5450, grad_fn=<DivBackward0>)\n",
      "tensor(1278.0573, grad_fn=<DivBackward0>)\n",
      "tensor(1262.7783, grad_fn=<DivBackward0>)\n",
      "tensor(1247.7004, grad_fn=<DivBackward0>)\n",
      "tensor(1232.8182, grad_fn=<DivBackward0>)\n",
      "tensor(1218.1267, grad_fn=<DivBackward0>)\n",
      "tensor(1203.6222, grad_fn=<DivBackward0>)\n",
      "tensor(1189.3010, grad_fn=<DivBackward0>)\n",
      "tensor(1175.1606, grad_fn=<DivBackward0>)\n",
      "tensor(1161.1980, grad_fn=<DivBackward0>)\n",
      "tensor(1147.4105, grad_fn=<DivBackward0>)\n",
      "tensor(1133.7963, grad_fn=<DivBackward0>)\n",
      "tensor(1120.3527, grad_fn=<DivBackward0>)\n",
      "tensor(1107.0774, grad_fn=<DivBackward0>)\n",
      "tensor(1093.9675, grad_fn=<DivBackward0>)\n",
      "tensor(1081.0225, grad_fn=<DivBackward0>)\n",
      "tensor(1068.2390, grad_fn=<DivBackward0>)\n",
      "tensor(1055.6156, grad_fn=<DivBackward0>)\n",
      "tensor(1043.1499, grad_fn=<DivBackward0>)\n",
      "tensor(1030.8400, grad_fn=<DivBackward0>)\n",
      "tensor(1018.6841, grad_fn=<DivBackward0>)\n",
      "tensor(1006.6799, grad_fn=<DivBackward0>)\n",
      "tensor(994.8259, grad_fn=<DivBackward0>)\n",
      "tensor(983.1198, grad_fn=<DivBackward0>)\n",
      "tensor(971.5601, grad_fn=<DivBackward0>)\n",
      "tensor(960.1447, grad_fn=<DivBackward0>)\n",
      "tensor(948.8718, grad_fn=<DivBackward0>)\n",
      "tensor(937.7399, grad_fn=<DivBackward0>)\n",
      "tensor(926.7469, grad_fn=<DivBackward0>)\n",
      "tensor(915.8910, grad_fn=<DivBackward0>)\n",
      "tensor(905.1708, grad_fn=<DivBackward0>)\n",
      "tensor(894.5844, grad_fn=<DivBackward0>)\n",
      "tensor(884.1300, grad_fn=<DivBackward0>)\n",
      "tensor(873.8062, grad_fn=<DivBackward0>)\n",
      "tensor(863.6111, grad_fn=<DivBackward0>)\n",
      "tensor(853.5431, grad_fn=<DivBackward0>)\n",
      "tensor(843.6008, grad_fn=<DivBackward0>)\n",
      "tensor(833.7825, grad_fn=<DivBackward0>)\n",
      "tensor(824.0868, grad_fn=<DivBackward0>)\n",
      "tensor(814.5117, grad_fn=<DivBackward0>)\n",
      "tensor(805.0562, grad_fn=<DivBackward0>)\n",
      "tensor(795.7184, grad_fn=<DivBackward0>)\n",
      "tensor(786.4969, grad_fn=<DivBackward0>)\n",
      "tensor(777.3905, grad_fn=<DivBackward0>)\n",
      "tensor(768.3976, grad_fn=<DivBackward0>)\n",
      "tensor(759.5167, grad_fn=<DivBackward0>)\n",
      "tensor(750.7463, grad_fn=<DivBackward0>)\n",
      "tensor(742.0851, grad_fn=<DivBackward0>)\n",
      "tensor(733.5320, grad_fn=<DivBackward0>)\n",
      "tensor(725.0854, grad_fn=<DivBackward0>)\n",
      "tensor(716.7438, grad_fn=<DivBackward0>)\n",
      "tensor(708.5059, grad_fn=<DivBackward0>)\n",
      "tensor(700.3707, grad_fn=<DivBackward0>)\n",
      "tensor(692.3367, grad_fn=<DivBackward0>)\n",
      "tensor(684.4026, grad_fn=<DivBackward0>)\n",
      "tensor(676.5674, grad_fn=<DivBackward0>)\n",
      "tensor(668.8293, grad_fn=<DivBackward0>)\n",
      "tensor(661.1876, grad_fn=<DivBackward0>)\n",
      "tensor(653.6410, grad_fn=<DivBackward0>)\n",
      "tensor(646.1882, grad_fn=<DivBackward0>)\n",
      "tensor(638.8278, grad_fn=<DivBackward0>)\n",
      "tensor(631.5590, grad_fn=<DivBackward0>)\n",
      "tensor(624.3805, grad_fn=<DivBackward0>)\n",
      "tensor(617.2913, grad_fn=<DivBackward0>)\n",
      "tensor(610.2899, grad_fn=<DivBackward0>)\n",
      "tensor(603.3758, grad_fn=<DivBackward0>)\n",
      "tensor(596.5472, grad_fn=<DivBackward0>)\n",
      "tensor(589.8036, grad_fn=<DivBackward0>)\n",
      "tensor(583.1435, grad_fn=<DivBackward0>)\n",
      "tensor(576.5661, grad_fn=<DivBackward0>)\n",
      "tensor(570.0703, grad_fn=<DivBackward0>)\n",
      "tensor(563.6552, grad_fn=<DivBackward0>)\n",
      "tensor(557.3195, grad_fn=<DivBackward0>)\n",
      "tensor(551.0624, grad_fn=<DivBackward0>)\n",
      "tensor(544.8830, grad_fn=<DivBackward0>)\n",
      "tensor(538.7800, grad_fn=<DivBackward0>)\n",
      "tensor(532.7528, grad_fn=<DivBackward0>)\n",
      "tensor(526.8002, grad_fn=<DivBackward0>)\n",
      "tensor(520.9211, grad_fn=<DivBackward0>)\n",
      "tensor(515.1151, grad_fn=<DivBackward0>)\n",
      "tensor(509.3809, grad_fn=<DivBackward0>)\n",
      "tensor(503.7178, grad_fn=<DivBackward0>)\n",
      "tensor(498.1247, grad_fn=<DivBackward0>)\n",
      "tensor(492.6008, grad_fn=<DivBackward0>)\n",
      "tensor(487.1452, grad_fn=<DivBackward0>)\n",
      "tensor(481.7571, grad_fn=<DivBackward0>)\n",
      "tensor(476.4355, grad_fn=<DivBackward0>)\n",
      "tensor(471.1800, grad_fn=<DivBackward0>)\n",
      "tensor(465.9893, grad_fn=<DivBackward0>)\n",
      "tensor(460.8626, grad_fn=<DivBackward0>)\n",
      "tensor(455.7994, grad_fn=<DivBackward0>)\n",
      "tensor(450.7987, grad_fn=<DivBackward0>)\n",
      "tensor(445.8598, grad_fn=<DivBackward0>)\n",
      "tensor(440.9818, grad_fn=<DivBackward0>)\n",
      "tensor(436.1640, grad_fn=<DivBackward0>)\n",
      "tensor(431.4057, grad_fn=<DivBackward0>)\n",
      "tensor(426.7061, grad_fn=<DivBackward0>)\n",
      "tensor(422.0645, grad_fn=<DivBackward0>)\n",
      "tensor(417.4801, grad_fn=<DivBackward0>)\n",
      "tensor(412.9522, grad_fn=<DivBackward0>)\n",
      "tensor(408.4800, grad_fn=<DivBackward0>)\n",
      "tensor(404.0630, grad_fn=<DivBackward0>)\n",
      "tensor(399.7004, grad_fn=<DivBackward0>)\n",
      "tensor(395.3916, grad_fn=<DivBackward0>)\n",
      "tensor(391.1357, grad_fn=<DivBackward0>)\n",
      "tensor(386.9322, grad_fn=<DivBackward0>)\n",
      "tensor(382.7805, grad_fn=<DivBackward0>)\n",
      "tensor(378.6797, grad_fn=<DivBackward0>)\n",
      "tensor(374.6296, grad_fn=<DivBackward0>)\n",
      "tensor(370.6292, grad_fn=<DivBackward0>)\n",
      "tensor(366.6778, grad_fn=<DivBackward0>)\n",
      "tensor(362.7750, grad_fn=<DivBackward0>)\n",
      "tensor(358.9202, grad_fn=<DivBackward0>)\n",
      "tensor(355.1127, grad_fn=<DivBackward0>)\n",
      "tensor(351.3521, grad_fn=<DivBackward0>)\n",
      "tensor(347.6375, grad_fn=<DivBackward0>)\n",
      "tensor(343.9684, grad_fn=<DivBackward0>)\n",
      "tensor(340.3445, grad_fn=<DivBackward0>)\n",
      "tensor(336.7650, grad_fn=<DivBackward0>)\n",
      "tensor(333.2292, grad_fn=<DivBackward0>)\n",
      "tensor(329.7368, grad_fn=<DivBackward0>)\n",
      "tensor(326.2872, grad_fn=<DivBackward0>)\n",
      "tensor(322.8798, grad_fn=<DivBackward0>)\n",
      "tensor(319.5142, grad_fn=<DivBackward0>)\n",
      "tensor(316.1897, grad_fn=<DivBackward0>)\n",
      "tensor(312.9059, grad_fn=<DivBackward0>)\n",
      "tensor(309.6623, grad_fn=<DivBackward0>)\n",
      "tensor(306.4583, grad_fn=<DivBackward0>)\n",
      "tensor(303.2935, grad_fn=<DivBackward0>)\n",
      "tensor(300.1673, grad_fn=<DivBackward0>)\n",
      "tensor(297.0793, grad_fn=<DivBackward0>)\n",
      "tensor(294.0290, grad_fn=<DivBackward0>)\n",
      "tensor(291.0161, grad_fn=<DivBackward0>)\n",
      "tensor(288.0396, grad_fn=<DivBackward0>)\n",
      "tensor(285.0996, grad_fn=<DivBackward0>)\n",
      "tensor(282.1954, grad_fn=<DivBackward0>)\n",
      "tensor(279.3266, grad_fn=<DivBackward0>)\n",
      "tensor(276.4928, grad_fn=<DivBackward0>)\n",
      "tensor(273.6935, grad_fn=<DivBackward0>)\n",
      "tensor(270.9281, grad_fn=<DivBackward0>)\n",
      "tensor(268.1964, grad_fn=<DivBackward0>)\n",
      "tensor(265.4981, grad_fn=<DivBackward0>)\n",
      "tensor(262.8324, grad_fn=<DivBackward0>)\n",
      "tensor(260.1992, grad_fn=<DivBackward0>)\n",
      "tensor(257.5979, grad_fn=<DivBackward0>)\n",
      "tensor(255.0282, grad_fn=<DivBackward0>)\n",
      "tensor(252.4898, grad_fn=<DivBackward0>)\n",
      "tensor(249.9821, grad_fn=<DivBackward0>)\n",
      "tensor(247.5049, grad_fn=<DivBackward0>)\n",
      "tensor(245.0576, grad_fn=<DivBackward0>)\n",
      "tensor(242.6401, grad_fn=<DivBackward0>)\n",
      "tensor(240.2517, grad_fn=<DivBackward0>)\n",
      "tensor(237.8922, grad_fn=<DivBackward0>)\n",
      "tensor(235.5615, grad_fn=<DivBackward0>)\n",
      "tensor(233.2588, grad_fn=<DivBackward0>)\n",
      "tensor(230.9840, grad_fn=<DivBackward0>)\n",
      "tensor(228.7366, grad_fn=<DivBackward0>)\n",
      "tensor(226.5164, grad_fn=<DivBackward0>)\n",
      "tensor(224.3230, grad_fn=<DivBackward0>)\n",
      "tensor(222.1561, grad_fn=<DivBackward0>)\n",
      "tensor(220.0153, grad_fn=<DivBackward0>)\n",
      "tensor(217.9002, grad_fn=<DivBackward0>)\n",
      "tensor(215.8107, grad_fn=<DivBackward0>)\n",
      "tensor(213.7464, grad_fn=<DivBackward0>)\n",
      "tensor(211.7070, grad_fn=<DivBackward0>)\n",
      "tensor(209.6920, grad_fn=<DivBackward0>)\n",
      "tensor(207.7012, grad_fn=<DivBackward0>)\n",
      "tensor(205.7344, grad_fn=<DivBackward0>)\n",
      "tensor(203.7912, grad_fn=<DivBackward0>)\n",
      "tensor(201.8713, grad_fn=<DivBackward0>)\n",
      "tensor(199.9745, grad_fn=<DivBackward0>)\n",
      "tensor(198.1004, grad_fn=<DivBackward0>)\n",
      "tensor(196.2489, grad_fn=<DivBackward0>)\n",
      "tensor(194.4194, grad_fn=<DivBackward0>)\n",
      "tensor(192.6118, grad_fn=<DivBackward0>)\n",
      "tensor(190.8260, grad_fn=<DivBackward0>)\n",
      "tensor(189.0614, grad_fn=<DivBackward0>)\n",
      "tensor(187.3180, grad_fn=<DivBackward0>)\n",
      "tensor(185.5954, grad_fn=<DivBackward0>)\n",
      "tensor(183.8933, grad_fn=<DivBackward0>)\n",
      "tensor(182.2116, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(180.5499, grad_fn=<DivBackward0>)\n",
      "tensor(178.9080, grad_fn=<DivBackward0>)\n",
      "tensor(177.2857, grad_fn=<DivBackward0>)\n",
      "tensor(175.6826, grad_fn=<DivBackward0>)\n",
      "tensor(174.0987, grad_fn=<DivBackward0>)\n",
      "tensor(172.5336, grad_fn=<DivBackward0>)\n",
      "tensor(170.9871, grad_fn=<DivBackward0>)\n",
      "tensor(169.4589, grad_fn=<DivBackward0>)\n",
      "tensor(167.9489, grad_fn=<DivBackward0>)\n",
      "tensor(166.4567, grad_fn=<DivBackward0>)\n",
      "tensor(164.9823, grad_fn=<DivBackward0>)\n",
      "tensor(163.5253, grad_fn=<DivBackward0>)\n",
      "tensor(162.0856, grad_fn=<DivBackward0>)\n",
      "tensor(160.6629, grad_fn=<DivBackward0>)\n",
      "tensor(159.2570, grad_fn=<DivBackward0>)\n",
      "tensor(157.8676, grad_fn=<DivBackward0>)\n",
      "tensor(156.4947, grad_fn=<DivBackward0>)\n",
      "tensor(155.1380, grad_fn=<DivBackward0>)\n",
      "tensor(153.7973, grad_fn=<DivBackward0>)\n",
      "tensor(152.4723, grad_fn=<DivBackward0>)\n",
      "tensor(151.1631, grad_fn=<DivBackward0>)\n",
      "tensor(149.8691, grad_fn=<DivBackward0>)\n",
      "tensor(148.5904, grad_fn=<DivBackward0>)\n",
      "tensor(147.3266, grad_fn=<DivBackward0>)\n",
      "tensor(146.0778, grad_fn=<DivBackward0>)\n",
      "tensor(144.8435, grad_fn=<DivBackward0>)\n",
      "tensor(143.6237, grad_fn=<DivBackward0>)\n",
      "tensor(142.4182, grad_fn=<DivBackward0>)\n",
      "tensor(141.2268, grad_fn=<DivBackward0>)\n",
      "tensor(140.0493, grad_fn=<DivBackward0>)\n",
      "tensor(138.8856, grad_fn=<DivBackward0>)\n",
      "tensor(137.7354, grad_fn=<DivBackward0>)\n",
      "tensor(136.5987, grad_fn=<DivBackward0>)\n",
      "tensor(135.4752, grad_fn=<DivBackward0>)\n",
      "tensor(134.3647, grad_fn=<DivBackward0>)\n",
      "tensor(133.2672, grad_fn=<DivBackward0>)\n",
      "tensor(132.1825, grad_fn=<DivBackward0>)\n",
      "tensor(131.1103, grad_fn=<DivBackward0>)\n",
      "tensor(130.0506, grad_fn=<DivBackward0>)\n",
      "tensor(129.0031, grad_fn=<DivBackward0>)\n",
      "tensor(127.9678, grad_fn=<DivBackward0>)\n",
      "tensor(126.9445, grad_fn=<DivBackward0>)\n",
      "tensor(125.9330, grad_fn=<DivBackward0>)\n",
      "tensor(124.9332, grad_fn=<DivBackward0>)\n",
      "tensor(123.9449, grad_fn=<DivBackward0>)\n",
      "tensor(122.9680, grad_fn=<DivBackward0>)\n",
      "tensor(122.0023, grad_fn=<DivBackward0>)\n",
      "tensor(121.0478, grad_fn=<DivBackward0>)\n",
      "tensor(120.1043, grad_fn=<DivBackward0>)\n",
      "tensor(119.1715, grad_fn=<DivBackward0>)\n",
      "tensor(118.2495, grad_fn=<DivBackward0>)\n",
      "tensor(117.3381, grad_fn=<DivBackward0>)\n",
      "tensor(116.4371, grad_fn=<DivBackward0>)\n",
      "tensor(115.5464, grad_fn=<DivBackward0>)\n",
      "tensor(114.6658, grad_fn=<DivBackward0>)\n",
      "tensor(113.7954, grad_fn=<DivBackward0>)\n",
      "tensor(112.9348, grad_fn=<DivBackward0>)\n",
      "tensor(112.0841, grad_fn=<DivBackward0>)\n",
      "tensor(111.2430, grad_fn=<DivBackward0>)\n",
      "tensor(110.4115, grad_fn=<DivBackward0>)\n",
      "tensor(109.5894, grad_fn=<DivBackward0>)\n",
      "tensor(108.7768, grad_fn=<DivBackward0>)\n",
      "tensor(107.9732, grad_fn=<DivBackward0>)\n",
      "tensor(107.1788, grad_fn=<DivBackward0>)\n",
      "tensor(106.3933, grad_fn=<DivBackward0>)\n",
      "tensor(105.6168, grad_fn=<DivBackward0>)\n",
      "tensor(104.8490, grad_fn=<DivBackward0>)\n",
      "tensor(104.0898, grad_fn=<DivBackward0>)\n",
      "tensor(103.3392, grad_fn=<DivBackward0>)\n",
      "tensor(102.5970, grad_fn=<DivBackward0>)\n",
      "tensor(101.8632, grad_fn=<DivBackward0>)\n",
      "tensor(101.1376, grad_fn=<DivBackward0>)\n",
      "tensor(100.4201, grad_fn=<DivBackward0>)\n",
      "tensor(99.7107, grad_fn=<DivBackward0>)\n",
      "tensor(99.0091, grad_fn=<DivBackward0>)\n",
      "tensor(98.3154, grad_fn=<DivBackward0>)\n",
      "tensor(97.6295, grad_fn=<DivBackward0>)\n",
      "tensor(96.9512, grad_fn=<DivBackward0>)\n",
      "tensor(96.2804, grad_fn=<DivBackward0>)\n",
      "tensor(95.6171, grad_fn=<DivBackward0>)\n",
      "tensor(94.9611, grad_fn=<DivBackward0>)\n",
      "tensor(94.3124, grad_fn=<DivBackward0>)\n",
      "tensor(93.6709, grad_fn=<DivBackward0>)\n",
      "tensor(93.0365, grad_fn=<DivBackward0>)\n",
      "tensor(92.4091, grad_fn=<DivBackward0>)\n",
      "tensor(91.7885, grad_fn=<DivBackward0>)\n",
      "tensor(91.1748, grad_fn=<DivBackward0>)\n",
      "tensor(90.5679, grad_fn=<DivBackward0>)\n",
      "tensor(89.9676, grad_fn=<DivBackward0>)\n",
      "tensor(89.3739, grad_fn=<DivBackward0>)\n",
      "tensor(88.7867, grad_fn=<DivBackward0>)\n",
      "tensor(88.2059, grad_fn=<DivBackward0>)\n",
      "tensor(87.6315, grad_fn=<DivBackward0>)\n",
      "tensor(87.0633, grad_fn=<DivBackward0>)\n",
      "tensor(86.5013, grad_fn=<DivBackward0>)\n",
      "tensor(85.9454, grad_fn=<DivBackward0>)\n",
      "tensor(85.3956, grad_fn=<DivBackward0>)\n",
      "tensor(84.8516, grad_fn=<DivBackward0>)\n",
      "tensor(84.3136, grad_fn=<DivBackward0>)\n",
      "tensor(83.7814, grad_fn=<DivBackward0>)\n",
      "tensor(83.2549, grad_fn=<DivBackward0>)\n",
      "tensor(82.7341, grad_fn=<DivBackward0>)\n",
      "tensor(82.2189, grad_fn=<DivBackward0>)\n",
      "tensor(81.7093, grad_fn=<DivBackward0>)\n",
      "tensor(81.2050, grad_fn=<DivBackward0>)\n",
      "tensor(80.7062, grad_fn=<DivBackward0>)\n",
      "tensor(80.2128, grad_fn=<DivBackward0>)\n",
      "tensor(79.7245, grad_fn=<DivBackward0>)\n",
      "tensor(79.2415, grad_fn=<DivBackward0>)\n",
      "tensor(78.7636, grad_fn=<DivBackward0>)\n",
      "tensor(78.2908, grad_fn=<DivBackward0>)\n",
      "tensor(77.8231, grad_fn=<DivBackward0>)\n",
      "tensor(77.3601, grad_fn=<DivBackward0>)\n",
      "tensor(76.9022, grad_fn=<DivBackward0>)\n",
      "tensor(76.4490, grad_fn=<DivBackward0>)\n",
      "tensor(76.0006, grad_fn=<DivBackward0>)\n",
      "tensor(75.5570, grad_fn=<DivBackward0>)\n",
      "tensor(75.1180, grad_fn=<DivBackward0>)\n",
      "tensor(74.6835, grad_fn=<DivBackward0>)\n",
      "tensor(74.2537, grad_fn=<DivBackward0>)\n",
      "tensor(73.8283, grad_fn=<DivBackward0>)\n",
      "tensor(73.4073, grad_fn=<DivBackward0>)\n",
      "tensor(72.9907, grad_fn=<DivBackward0>)\n",
      "tensor(72.5785, grad_fn=<DivBackward0>)\n",
      "tensor(72.1705, grad_fn=<DivBackward0>)\n",
      "tensor(71.7667, grad_fn=<DivBackward0>)\n",
      "tensor(71.3670, grad_fn=<DivBackward0>)\n",
      "tensor(70.9715, grad_fn=<DivBackward0>)\n",
      "tensor(70.5801, grad_fn=<DivBackward0>)\n",
      "tensor(70.1927, grad_fn=<DivBackward0>)\n",
      "tensor(69.8093, grad_fn=<DivBackward0>)\n",
      "tensor(69.4297, grad_fn=<DivBackward0>)\n",
      "tensor(69.0540, grad_fn=<DivBackward0>)\n",
      "tensor(68.6822, grad_fn=<DivBackward0>)\n",
      "tensor(68.3140, grad_fn=<DivBackward0>)\n",
      "tensor(67.9497, grad_fn=<DivBackward0>)\n",
      "tensor(67.5890, grad_fn=<DivBackward0>)\n",
      "tensor(67.2320, grad_fn=<DivBackward0>)\n",
      "tensor(66.8786, grad_fn=<DivBackward0>)\n",
      "tensor(66.5287, grad_fn=<DivBackward0>)\n",
      "tensor(66.1824, grad_fn=<DivBackward0>)\n",
      "tensor(65.8394, grad_fn=<DivBackward0>)\n",
      "tensor(65.5000, grad_fn=<DivBackward0>)\n",
      "tensor(65.1639, grad_fn=<DivBackward0>)\n",
      "tensor(64.8311, grad_fn=<DivBackward0>)\n",
      "tensor(64.5016, grad_fn=<DivBackward0>)\n",
      "tensor(64.1755, grad_fn=<DivBackward0>)\n",
      "tensor(63.8525, grad_fn=<DivBackward0>)\n",
      "tensor(63.5327, grad_fn=<DivBackward0>)\n",
      "tensor(63.2161, grad_fn=<DivBackward0>)\n",
      "tensor(62.9025, grad_fn=<DivBackward0>)\n",
      "tensor(62.5920, grad_fn=<DivBackward0>)\n",
      "tensor(62.2845, grad_fn=<DivBackward0>)\n",
      "tensor(61.9801, grad_fn=<DivBackward0>)\n",
      "tensor(61.6786, grad_fn=<DivBackward0>)\n",
      "tensor(61.3800, grad_fn=<DivBackward0>)\n",
      "tensor(61.0843, grad_fn=<DivBackward0>)\n",
      "tensor(60.7915, grad_fn=<DivBackward0>)\n",
      "tensor(60.5015, grad_fn=<DivBackward0>)\n",
      "tensor(60.2142, grad_fn=<DivBackward0>)\n",
      "tensor(59.9298, grad_fn=<DivBackward0>)\n",
      "tensor(59.6479, grad_fn=<DivBackward0>)\n",
      "tensor(59.3689, grad_fn=<DivBackward0>)\n",
      "tensor(59.0925, grad_fn=<DivBackward0>)\n",
      "tensor(58.8187, grad_fn=<DivBackward0>)\n",
      "tensor(58.5474, grad_fn=<DivBackward0>)\n",
      "tensor(58.2788, grad_fn=<DivBackward0>)\n",
      "tensor(58.0126, grad_fn=<DivBackward0>)\n",
      "tensor(57.7490, grad_fn=<DivBackward0>)\n",
      "tensor(57.4879, grad_fn=<DivBackward0>)\n",
      "tensor(57.2291, grad_fn=<DivBackward0>)\n",
      "tensor(56.9728, grad_fn=<DivBackward0>)\n",
      "tensor(56.7189, grad_fn=<DivBackward0>)\n",
      "tensor(56.4672, grad_fn=<DivBackward0>)\n",
      "tensor(56.2180, grad_fn=<DivBackward0>)\n",
      "tensor(55.9711, grad_fn=<DivBackward0>)\n",
      "tensor(55.7263, grad_fn=<DivBackward0>)\n",
      "tensor(55.4839, grad_fn=<DivBackward0>)\n",
      "tensor(55.2436, grad_fn=<DivBackward0>)\n",
      "tensor(55.0056, grad_fn=<DivBackward0>)\n",
      "tensor(54.7696, grad_fn=<DivBackward0>)\n",
      "tensor(54.5358, grad_fn=<DivBackward0>)\n",
      "tensor(54.3042, grad_fn=<DivBackward0>)\n",
      "tensor(54.0747, grad_fn=<DivBackward0>)\n",
      "tensor(53.8472, grad_fn=<DivBackward0>)\n",
      "tensor(53.6216, grad_fn=<DivBackward0>)\n",
      "tensor(53.3982, grad_fn=<DivBackward0>)\n",
      "tensor(53.1767, grad_fn=<DivBackward0>)\n",
      "tensor(52.9572, grad_fn=<DivBackward0>)\n",
      "tensor(52.7397, grad_fn=<DivBackward0>)\n",
      "tensor(52.5241, grad_fn=<DivBackward0>)\n",
      "tensor(52.3102, grad_fn=<DivBackward0>)\n",
      "tensor(52.0984, grad_fn=<DivBackward0>)\n",
      "tensor(51.8883, grad_fn=<DivBackward0>)\n",
      "tensor(51.6802, grad_fn=<DivBackward0>)\n",
      "tensor(51.4738, grad_fn=<DivBackward0>)\n",
      "tensor(51.2691, grad_fn=<DivBackward0>)\n",
      "tensor(51.0663, grad_fn=<DivBackward0>)\n",
      "tensor(50.8652, grad_fn=<DivBackward0>)\n",
      "tensor(50.6659, grad_fn=<DivBackward0>)\n",
      "tensor(50.4682, grad_fn=<DivBackward0>)\n",
      "tensor(50.2722, grad_fn=<DivBackward0>)\n",
      "tensor(50.0779, grad_fn=<DivBackward0>)\n",
      "tensor(49.8852, grad_fn=<DivBackward0>)\n",
      "tensor(49.6942, grad_fn=<DivBackward0>)\n",
      "tensor(49.5047, grad_fn=<DivBackward0>)\n",
      "tensor(49.3169, grad_fn=<DivBackward0>)\n",
      "tensor(49.1306, grad_fn=<DivBackward0>)\n",
      "tensor(48.9459, grad_fn=<DivBackward0>)\n",
      "tensor(48.7627, grad_fn=<DivBackward0>)\n",
      "tensor(48.5810, grad_fn=<DivBackward0>)\n",
      "tensor(48.4008, grad_fn=<DivBackward0>)\n",
      "tensor(48.2221, grad_fn=<DivBackward0>)\n",
      "tensor(48.0448, grad_fn=<DivBackward0>)\n",
      "tensor(47.8690, grad_fn=<DivBackward0>)\n",
      "tensor(47.6947, grad_fn=<DivBackward0>)\n",
      "tensor(47.5217, grad_fn=<DivBackward0>)\n",
      "tensor(47.3502, grad_fn=<DivBackward0>)\n",
      "tensor(47.1800, grad_fn=<DivBackward0>)\n",
      "tensor(47.0112, grad_fn=<DivBackward0>)\n",
      "tensor(46.8437, grad_fn=<DivBackward0>)\n",
      "tensor(46.6775, grad_fn=<DivBackward0>)\n",
      "tensor(46.5127, grad_fn=<DivBackward0>)\n",
      "tensor(46.3492, grad_fn=<DivBackward0>)\n",
      "tensor(46.1870, grad_fn=<DivBackward0>)\n",
      "tensor(46.0260, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.8663, grad_fn=<DivBackward0>)\n",
      "tensor(45.7078, grad_fn=<DivBackward0>)\n",
      "tensor(45.5506, grad_fn=<DivBackward0>)\n",
      "tensor(45.3946, grad_fn=<DivBackward0>)\n",
      "tensor(45.2398, grad_fn=<DivBackward0>)\n",
      "tensor(45.0861, grad_fn=<DivBackward0>)\n",
      "tensor(44.9337, grad_fn=<DivBackward0>)\n",
      "tensor(44.7824, grad_fn=<DivBackward0>)\n",
      "tensor(44.6323, grad_fn=<DivBackward0>)\n",
      "tensor(44.4833, grad_fn=<DivBackward0>)\n",
      "tensor(44.3354, grad_fn=<DivBackward0>)\n",
      "tensor(44.1886, grad_fn=<DivBackward0>)\n",
      "tensor(44.0429, grad_fn=<DivBackward0>)\n",
      "tensor(43.8983, grad_fn=<DivBackward0>)\n",
      "tensor(43.7548, grad_fn=<DivBackward0>)\n",
      "tensor(43.6123, grad_fn=<DivBackward0>)\n",
      "tensor(43.4708, grad_fn=<DivBackward0>)\n",
      "tensor(43.3305, grad_fn=<DivBackward0>)\n",
      "tensor(43.1912, grad_fn=<DivBackward0>)\n",
      "tensor(43.0528, grad_fn=<DivBackward0>)\n",
      "tensor(42.9154, grad_fn=<DivBackward0>)\n",
      "tensor(42.7790, grad_fn=<DivBackward0>)\n",
      "tensor(42.6437, grad_fn=<DivBackward0>)\n",
      "tensor(42.5092, grad_fn=<DivBackward0>)\n",
      "tensor(42.3757, grad_fn=<DivBackward0>)\n",
      "tensor(42.2432, grad_fn=<DivBackward0>)\n",
      "tensor(42.1117, grad_fn=<DivBackward0>)\n",
      "tensor(41.9810, grad_fn=<DivBackward0>)\n",
      "tensor(41.8513, grad_fn=<DivBackward0>)\n",
      "tensor(41.7224, grad_fn=<DivBackward0>)\n",
      "tensor(41.5944, grad_fn=<DivBackward0>)\n",
      "tensor(41.4674, grad_fn=<DivBackward0>)\n",
      "tensor(41.3412, grad_fn=<DivBackward0>)\n",
      "tensor(41.2159, grad_fn=<DivBackward0>)\n",
      "tensor(41.0914, grad_fn=<DivBackward0>)\n",
      "tensor(40.9678, grad_fn=<DivBackward0>)\n",
      "tensor(40.8450, grad_fn=<DivBackward0>)\n",
      "tensor(40.7231, grad_fn=<DivBackward0>)\n",
      "tensor(40.6020, grad_fn=<DivBackward0>)\n",
      "tensor(40.4816, grad_fn=<DivBackward0>)\n",
      "tensor(40.3621, grad_fn=<DivBackward0>)\n",
      "tensor(40.2434, grad_fn=<DivBackward0>)\n",
      "tensor(40.1254, grad_fn=<DivBackward0>)\n",
      "tensor(40.0082, grad_fn=<DivBackward0>)\n",
      "tensor(39.8918, grad_fn=<DivBackward0>)\n",
      "tensor(39.7761, grad_fn=<DivBackward0>)\n",
      "tensor(39.6612, grad_fn=<DivBackward0>)\n",
      "tensor(39.5471, grad_fn=<DivBackward0>)\n",
      "tensor(39.4336, grad_fn=<DivBackward0>)\n",
      "tensor(39.3209, grad_fn=<DivBackward0>)\n",
      "tensor(39.2089, grad_fn=<DivBackward0>)\n",
      "tensor(39.0976, grad_fn=<DivBackward0>)\n",
      "tensor(38.9870, grad_fn=<DivBackward0>)\n",
      "tensor(38.8771, grad_fn=<DivBackward0>)\n",
      "tensor(38.7680, grad_fn=<DivBackward0>)\n",
      "tensor(38.6595, grad_fn=<DivBackward0>)\n",
      "tensor(38.5516, grad_fn=<DivBackward0>)\n",
      "tensor(38.4444, grad_fn=<DivBackward0>)\n",
      "tensor(38.3379, grad_fn=<DivBackward0>)\n",
      "tensor(38.2320, grad_fn=<DivBackward0>)\n",
      "tensor(38.1268, grad_fn=<DivBackward0>)\n",
      "tensor(38.0222, grad_fn=<DivBackward0>)\n",
      "tensor(37.9183, grad_fn=<DivBackward0>)\n",
      "tensor(37.8149, grad_fn=<DivBackward0>)\n",
      "tensor(37.7122, grad_fn=<DivBackward0>)\n",
      "tensor(37.6101, grad_fn=<DivBackward0>)\n",
      "tensor(37.5085, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for i in range(0,epochs): \n",
    "    prediction = model(inputs) #step 3 \n",
    "    loss = mse(prediction, targets)#step 4\n",
    "    loss.backward() #step 5\n",
    "    with torch.no_grad(): \n",
    "        w-= w.grad*1e-5 \n",
    "        b-= b.grad*1e-5\n",
    "    #resetting gradients to 0\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    print(loss) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 500 epochs, our loss is 37.5 units which is way lower than what we started with.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
